# -*- coding: utf-8 -*-
"""TEXT-TO-IMAGE GENERATOR USING REAL DATASET (CIFAR-10).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-dexi_SVO0-ZEGDhNf6j3BoKzuxLpFVt
"""

# ================================================================
# TEXT-TO-IMAGE GENERATOR USING REAL DATASET (CIFAR-10)
# ================================================================
# Enhanced version using actual image datasets
# Student: Mahmoud Ahmed Abd Elazim
# Submitted to: Prof. Mahmoud Mahdy
# ================================================================

# ================================================================
# STEP 1: INSTALL REQUIRED PACKAGES
# ================================================================
print("="*70)
print("INSTALLING REQUIRED PACKAGES")
print("="*70)

!pip install torch torchvision numpy matplotlib Pillow tqdm -q

print("✓ Packages installed successfully!")

# ================================================================
# STEP 2: IMPORT ALL LIBRARIES
# ================================================================
print("\n" + "="*70)
print("IMPORTING LIBRARIES")
print("="*70)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision import datasets
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import random
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

print("✓ All libraries imported successfully!")

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\n✓ Using device: {device}")
if torch.cuda.is_available():
    print(f"✓ GPU Name: {torch.cuda.get_device_name(0)}")
    print(f"✓ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

print(f"✓ PyTorch version: {torch.__version__}")

# ================================================================
# STEP 3: DOWNLOAD CIFAR-10 DATASET
# ================================================================
print("\n" + "="*70)
print("DOWNLOADING CIFAR-10 DATASET")
print("="*70)

# CIFAR-10 class names
class_names = [
    'airplane', 'automobile', 'bird', 'cat', 'deer',
    'dog', 'frog', 'horse', 'ship', 'truck'
]

print("Downloading CIFAR-10 training set...")
train_dataset_raw = datasets.CIFAR10(
    root='./data',
    train=True,
    download=True,
    transform=None
)

print("Downloading CIFAR-10 test set...")
test_dataset_raw = datasets.CIFAR10(
    root='./data',
    train=False,
    download=True,
    transform=None
)

print(f"\n✓ Training samples: {len(train_dataset_raw)}")
print(f"✓ Test samples: {len(test_dataset_raw)}")
print(f"✓ Classes: {', '.join(class_names)}")

# ================================================================
# STEP 4: CREATE TEXT DESCRIPTIONS
# ================================================================
print("\n" + "="*70)
print("CREATING TEXT DESCRIPTIONS")
print("="*70)

# Define color variations for each class
color_variations = {
    'airplane': ['gray', 'white', 'blue', 'red'],
    'automobile': ['red', 'blue', 'black', 'white'],
    'bird': ['colorful', 'brown', 'blue', 'red'],
    'cat': ['orange', 'gray', 'white', 'black'],
    'deer': ['brown', 'tan', 'beige'],
    'dog': ['brown', 'black', 'white', 'golden'],
    'frog': ['green', 'brown', 'yellow'],
    'horse': ['brown', 'black', 'white'],
    'ship': ['white', 'gray', 'blue', 'red'],
    'truck': ['red', 'blue', 'white', 'yellow']
}

def generate_description(class_id):
    """Generate varied descriptions for each class"""
    class_name = class_names[class_id]
    color = random.choice(color_variations[class_name])

    # Varied description templates
    templates = [
        f"{color} {class_name}",
        f"a {color} {class_name}",
        f"small {color} {class_name}",
        f"{class_name} in {color}",
    ]
    return random.choice(templates)

# Test description generation
print("\nSample descriptions:")
for i in range(5):
    desc = generate_description(random.randint(0, 9))
    print(f"  {i+1}. {desc}")

# ================================================================
# STEP 5: BUILD VOCABULARY
# ================================================================
print("\n" + "="*70)
print("BUILDING VOCABULARY")
print("="*70)

def build_vocabulary():
    """Build vocabulary from all possible description words"""
    vocab = {'<PAD>': 0, '<UNK>': 1, 'a': 2, 'small': 3, 'in': 4}
    idx = 5

    # Add all class names
    for name in class_names:
        if name not in vocab:
            vocab[name] = idx
            idx += 1

    # Add all possible colors
    all_colors = set()
    for colors in color_variations.values():
        all_colors.update(colors)

    for color in all_colors:
        if color not in vocab:
            vocab[color] = idx
            idx += 1

    # Create reverse mapping
    idx2word = {v: k for k, v in vocab.items()}

    return vocab, idx2word

vocab, idx2word = build_vocabulary()
print(f"✓ Vocabulary size: {len(vocab)}")
print(f"✓ Words: {list(vocab.keys())}")

# ================================================================
# STEP 6: CREATE DATASET CLASS
# ================================================================
print("\n" + "="*70)
print("CREATING DATASET CLASS")
print("="*70)

class TextImageDataset(Dataset):
    """Dataset that pairs images with text descriptions"""

    def __init__(self, raw_dataset, vocab, img_size=32, augment=False):
        self.raw_dataset = raw_dataset
        self.vocab = vocab
        self.img_size = img_size

        # Image transformations
        if augment:
            self.transform = transforms.Compose([
                transforms.Resize((img_size, img_size)),
                transforms.RandomHorizontalFlip(),
                transforms.ColorJitter(brightness=0.2, contrast=0.2),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
            ])
        else:
            self.transform = transforms.Compose([
                transforms.Resize((img_size, img_size)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
            ])

    def encode_text(self, text, max_len=8):
        """Convert text to sequence of indices"""
        words = text.lower().split()
        indices = [self.vocab.get(word, 1) for word in words]

        # Pad or truncate
        if len(indices) < max_len:
            indices += [0] * (max_len - len(indices))
        else:
            indices = indices[:max_len]

        return indices

    def __len__(self):
        return len(self.raw_dataset)

    def __getitem__(self, idx):
        # Get image and label from raw dataset
        img, label = self.raw_dataset[idx]

        # Generate text description
        text = generate_description(label)

        # Transform image
        if isinstance(img, Image.Image):
            img = self.transform(img)
        else:
            img = self.transform(Image.fromarray(img))

        # Encode text
        text_encoded = self.encode_text(text)

        return torch.tensor(text_encoded, dtype=torch.long), img

# Create datasets
print("Creating training dataset...")
train_dataset = TextImageDataset(
    train_dataset_raw,
    vocab,
    img_size=32,
    augment=True
)

print("Creating test dataset...")
test_dataset = TextImageDataset(
    test_dataset_raw,
    vocab,
    img_size=32,
    augment=False
)

print(f"\n✓ Training samples: {len(train_dataset)}")
print(f"✓ Test samples: {len(test_dataset)}")

# ================================================================
# STEP 7: CREATE DATA LOADERS
# ================================================================
print("\n" + "="*70)
print("CREATING DATA LOADERS")
print("="*70)

BATCH_SIZE = 128

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=2,
    pin_memory=True if torch.cuda.is_available() else False
)

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True if torch.cuda.is_available() else False
)

print(f"✓ Batch size: {BATCH_SIZE}")
print(f"✓ Training batches: {len(train_loader)}")
print(f"✓ Test batches: {len(test_loader)}")

# ================================================================
# STEP 8: BUILD MODEL ARCHITECTURE
# ================================================================
print("\n" + "="*70)
print("BUILDING MODEL ARCHITECTURE")
print("="*70)

class EnhancedTextEncoder(nn.Module):
    """Improved text encoder with better capacity"""

    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128, num_layers=2):
        super(EnhancedTextEncoder, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            embed_dim,
            hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2 if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_dim, hidden_dim)
        self.activation = nn.ReLU()

    def forward(self, text):
        # text: (batch, seq_len)
        embedded = self.embedding(text)  # (batch, seq_len, embed_dim)
        _, (hidden, _) = self.lstm(embedded)  # (num_layers, batch, hidden_dim)

        # Use last layer's hidden state
        hidden = hidden[-1]  # (batch, hidden_dim)

        # Additional processing
        output = self.fc(hidden)
        output = self.activation(output)

        return output

class EnhancedImageDecoder(nn.Module):
    """Improved decoder for better image quality"""

    def __init__(self, input_dim=128, img_size=32):
        super(EnhancedImageDecoder, self).__init__()
        self.img_size = img_size

        # Project to higher dimension
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 128 * 4 * 4),
            nn.ReLU()
        )

        # For 32×32 output
        self.decoder = nn.Sequential(
            # 4×4 -> 8×8
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),

            # 8×8 -> 16×16
            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),

            # 16×16 -> 32×32
            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),
            nn.Tanh()  # Output in range [-1, 1]
        )

    def forward(self, x):
        x = self.fc(x)
        x = x.view(-1, 128, 4, 4)
        x = self.decoder(x)
        return x

class TextToImageModel(nn.Module):
    """Complete enhanced model"""

    def __init__(self, vocab_size, img_size=32):
        super(TextToImageModel, self).__init__()
        self.encoder = EnhancedTextEncoder(vocab_size, embed_dim=64, hidden_dim=128)
        self.decoder = EnhancedImageDecoder(input_dim=128, img_size=img_size)

    def forward(self, text):
        features = self.encoder(text)
        image = self.decoder(features)
        return image

# Create model
model = TextToImageModel(vocab_size=len(vocab), img_size=32).to(device)

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"✓ Total parameters: {total_params:,}")
print(f"✓ Trainable parameters: {trainable_params:,}")

# ================================================================
# STEP 9: SETUP TRAINING
# ================================================================
print("\n" + "="*70)
print("SETTING UP TRAINING")
print("="*70)

# Loss functions
criterion_mse = nn.MSELoss()
criterion_l1 = nn.L1Loss()

def combined_loss(pred, target, alpha=0.8):
    """Combine MSE and L1 loss for better quality"""
    mse_loss = criterion_mse(pred, target)
    l1_loss = criterion_l1(pred, target)
    return alpha * mse_loss + (1 - alpha) * l1_loss

# Optimizer
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)

# Learning rate scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=3
)

print("✓ Loss function: Combined (MSE + L1)")
print("✓ Optimizer: AdamW")
print("✓ Learning rate: 0.001")
print("✓ Scheduler: ReduceLROnPlateau")

# ================================================================
# STEP 10: TRAINING FUNCTIONS
# ================================================================
print("\n" + "="*70)
print("DEFINING TRAINING FUNCTIONS")
print("="*70)

def train_epoch(model, loader, optimizer, device):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    progress_bar = tqdm(loader, desc="Training", leave=False)

    for text, target_img in progress_bar:
        text = text.to(device)
        target_img = target_img.to(device)

        # Forward pass
        pred_img = model(text)
        loss = combined_loss(pred_img, target_img)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        total_loss += loss.item()
        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

    return total_loss / len(loader)

def validate(model, loader, device):
    """Validate the model"""
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for text, target_img in tqdm(loader, desc="Validation", leave=False):
            text = text.to(device)
            target_img = target_img.to(device)

            pred_img = model(text)
            loss = combined_loss(pred_img, target_img)
            total_loss += loss.item()

    return total_loss / len(loader)

print("✓ Training functions defined")

# ================================================================
# STEP 11: TRAIN THE MODEL
# ================================================================
print("\n" + "="*70)
print("STARTING TRAINING")
print("="*70)

NUM_EPOCHS = 30
train_losses = []
val_losses = []
best_val_loss = float('inf')

print(f"Training for {NUM_EPOCHS} epochs...\n")

for epoch in range(NUM_EPOCHS):
    print(f"\nEpoch {epoch+1}/{NUM_EPOCHS}")
    print("-" * 50)

    # Train
    train_loss = train_epoch(model, train_loader, optimizer, device)
    train_losses.append(train_loss)

    # Validate
    val_loss = validate(model, test_loader, device)
    val_losses.append(val_loss)

    # Update learning rate
    old_lr = optimizer.param_groups[0]['lr']
    scheduler.step(val_loss)
    new_lr = optimizer.param_groups[0]['lr']

    # Print results
    print(f"\nResults:")
    print(f"  Train Loss: {train_loss:.6f}")
    print(f"  Val Loss:   {val_loss:.6f}")
    print(f"  LR:         {new_lr:.6f}")

    if new_lr < old_lr:
        print(f"  ⚡ Learning rate reduced: {old_lr:.6f} → {new_lr:.6f}")

    # Save best model
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': val_loss,
            'vocab': vocab,
        }, 'best_model.pth')
        print(f"  ✓ New best model saved!")

print("\n" + "="*70)
print("TRAINING COMPLETED!")
print("="*70)

# ================================================================
# STEP 12: VISUALIZE TRAINING PROGRESS
# ================================================================
print("\n" + "="*70)
print("VISUALIZING RESULTS")
print("="*70)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss', linewidth=2, marker='o')
plt.plot(val_losses, label='Validation Loss', linewidth=2, marker='s')
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.title('Training Progress', fontsize=14, fontweight='bold')
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
epochs = list(range(1, len(val_losses) + 1))
plt.plot(epochs, val_losses, label='Val Loss', linewidth=2, color='orange', marker='o')
plt.axhline(y=best_val_loss, color='r', linestyle='--', alpha=0.5, label=f'Best: {best_val_loss:.4f}')
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Validation Loss', fontsize=12)
plt.title('Validation Loss Progress', fontsize=14, fontweight='bold')
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ================================================================
# STEP 13: LOAD BEST MODEL AND GENERATE SAMPLES
# ================================================================
print("\n" + "="*70)
print("GENERATING SAMPLE IMAGES")
print("="*70)

# Load best model
checkpoint = torch.load('best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

print(f"✓ Loaded best model from epoch {checkpoint['epoch']+1}")
print(f"✓ Best validation loss: {checkpoint['val_loss']:.6f}")

# Denormalization function
def denormalize(img):
    """Convert from [-1, 1] to [0, 1]"""
    return (img + 1) / 2

# Get a batch from test set
test_iter = iter(test_loader)
text_batch, target_batch = next(test_iter)

# Generate predictions
with torch.no_grad():
    pred_batch = model(text_batch.to(device)).cpu()

# Visualize results
num_samples = 8
fig, axes = plt.subplots(3, num_samples, figsize=(20, 8))

for i in range(num_samples):
    # Original image
    orig_img = target_batch[i].permute(1, 2, 0).numpy()
    orig_img = denormalize(orig_img)
    orig_img = np.clip(orig_img, 0, 1)
    axes[0, i].imshow(orig_img)
    axes[0, i].axis('off')
    if i == 0:
        axes[0, i].set_ylabel('Original', fontsize=12, fontweight='bold')

    # Generated image
    gen_img = pred_batch[i].permute(1, 2, 0).numpy()
    gen_img = denormalize(gen_img)
    gen_img = np.clip(gen_img, 0, 1)
    axes[1, i].imshow(gen_img)
    axes[1, i].axis('off')
    if i == 0:
        axes[1, i].set_ylabel('Generated', fontsize=12, fontweight='bold')

    # Text description
    text_indices = text_batch[i].tolist()
    words = [idx2word.get(idx, '') for idx in text_indices if idx != 0]
    description = ' '.join(words)

    axes[2, i].text(0.5, 0.5, description,
                    ha='center', va='center',
                    fontsize=10, wrap=True)
    axes[2, i].axis('off')
    if i == 0:
        axes[2, i].set_ylabel('Prompt', fontsize=12, fontweight='bold')

plt.suptitle('Text-to-Image Generation Results (CIFAR-10)',
             fontsize=16, fontweight='bold', y=0.98)
plt.tight_layout()
plt.show()

# ================================================================
# STEP 14: INTERACTIVE TESTING
# ================================================================
print("\n" + "="*70)
print("INTERACTIVE GENERATION")
print("="*70)

def generate_from_text(text, model, vocab, device):
    """Generate image from custom text description"""
    model.eval()

    # Encode text
    words = text.lower().split()
    indices = [vocab.get(word, 1) for word in words]

    # Pad to length 8
    if len(indices) < 8:
        indices += [0] * (8 - len(indices))
    else:
        indices = indices[:8]

    text_tensor = torch.tensor([indices], dtype=torch.long).to(device)

    # Generate
    with torch.no_grad():
        img = model(text_tensor).cpu()

    # Denormalize
    img = img[0].permute(1, 2, 0).numpy()
    img = denormalize(img)
    img = np.clip(img, 0, 1)

    return img

# Test with custom prompts
test_prompts = [
    "red automobile",
    "blue airplane",
    "brown dog",
    "green frog",
    "white ship",
    "black horse",
    "gray cat",
    "yellow truck"
]

fig, axes = plt.subplots(2, 4, figsize=(16, 8))
axes = axes.flatten()

for idx, prompt in enumerate(test_prompts):
    img = generate_from_text(prompt, model, vocab, device)
    axes[idx].imshow(img)
    axes[idx].set_title(f'"{prompt}"', fontsize=11, fontweight='bold')
    axes[idx].axis('off')

plt.suptitle('Custom Text Prompts Generation', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

# ================================================================
# STEP 15: FINAL SUMMARY
# ================================================================
print("\n" + "="*70)
print("PROJECT COMPLETED SUCCESSFULLY!")
print("="*70)

print(f"""
Summary:
--------
✓ Dataset: CIFAR-10 (60,000 real images)
✓ Model: Enhanced Encoder-Decoder Architecture
✓ Training: {NUM_EPOCHS} epochs completed
✓ Best Validation Loss: {best_val_loss:.6f}
✓ Model saved as: best_model.pth

Model Statistics:
-----------------
• Total Parameters: {total_params:,}
• Vocabulary Size: {len(vocab)}
• Image Resolution: 32×32 pixels
• Batch Size: {BATCH_SIZE}

Next Steps:
-----------
1. Try different text descriptions
2. Fine-tune hyperparameters
3. Experiment with larger images (64×64)
4. Try other datasets (Tiny ImageNet, COCO)
5. Implement GAN-based training

For Academic Report:
--------------------
✓ Real-world dataset used
✓ Complete training pipeline
✓ Proper validation methodology
✓ Publication-ready visualizations
✓ Reproducible results

Feom: Mahmoud Ahmed Abd Elazim
Instructor: Prof. Mahmoud Mahdy
""")